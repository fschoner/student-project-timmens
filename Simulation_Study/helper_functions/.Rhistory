if(model==6)
{
#Sample N random uniforms U
#Variable to store the samples from the mixture distribution
eps =  runif(N,-1,1)
#Sampling from the mixture
}
###Heteroscedastic error term
beta.vec  <- c(1,1.5,-1.5,1.5)
y         <- X %*% beta.vec + eps
##Solving for beta hat###
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
x.grid<-sort(X.2) #seq(min(X.2),max(X.2),le=N)
true_model<-beta.vec[1]+beta.vec[2]*x.grid+beta.vec[3]*x.grid^2+beta.vec[4]*x.grid^3
estim_model<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
plot(estim_model,type="l")
lines(true_model,type="l",lty=1,col="red")
####Calculate the covariance matrix###
#####calculate the fitted values#####
y.hat<- X %*% beta.hat
eps.hat<-y-X %*% beta.hat
###calculate the covariance matrix#
se<-(t(eps.hat)%*%(eps.hat))/(N-5)
cov<-se[1]*solve(t(X) %*% X)
new_data<-X[order(X[,2]),]
#new_data<-x.grid
var<-c()
for(i in 1:N)
{
var[i]=t(new_data[i,])%*%cov%*%(new_data[i,])
}
d<-2*(sqrt(var))
estim<-beta.hat[1]+beta.hat[2]*new_data[,2]+beta.hat[3]*new_data[,2]^2+beta.hat[4]*new_data[,2]^3
conf_low<-estim-d
conf_high<-estim+d
plot(estim,type="l")
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
######Bootstrapped confidence intervals#####################
B=N
b_sample<-matrix(NA,N,B)
beta.hat<-matrix(NA,length(X),B)
data<-cbind(y,X)
function_B<-matrix(NA,length(x.grid),B)
for(i in 1:B)
{
XB<- data[sample(nrow(data),replace=T, N), ]
beta.hat<-solve(t(XB[,2:5]) %*% XB[,2:5]) %*% t(XB[,2:5]) %*%XB[,1]
function_B[,i]<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
}
plot(function_B[,1])
plot(function_B[,2])
sorted<-t(apply(function_B,1,sort))
####Calculate the empirical quantiles####################
conf_highB<-sorted[,B*0.95]
conf_lowB<-sorted[,B*0.05]
###########################################
plot(estim,type="l",lty=3)
lines(conf_highB)
lines(conf_lowB)
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
lines(true_model,col="green")
####Calculate the coverage probabilities#################################
cov_prob_B<-true_model>conf_lowB&true_model<conf_highB
ratio1_B= sum(cov_prob_B)/length(x.grid)
cov_prob_A<-true_model>conf_low&true_model<conf_high
ratio1=sum(cov_prob_A)/length(x.grid)
###Calculate interval length##############################################
int_lengthB<-conf_highB-conf_lowB
int_length<-conf_high-conf_low
ratio2=sum(int_lengthB/int_length>1)####counts the number of times the bootstrapped interval length is larger than the analytical one#########
ratio2_int=mean(int_length)
ratioB_int=mean(int_lengthB)
return(list(ratio2=ratio2,ratio1_B=ratio1_B,ratio1=ratio1,ratio2_int=ratio2_int,ratioB_int=ratioB_int))
}
##########################################################################
data.conf<-function(model,N)
{
#########Simulating OLS samples and plot the regression lines####
#set.seed(32323)
## Two explanatory variables plus an intercept:
#N         <- 200 # Number of observations
X.1       <- rep(1, N)
X.2      <- rnorm(N, mean=0, sd=1) # (pseudo) random numbers form a normal distr
X         <- cbind(X.1, X.2,X.2^2,X.2^3)
###Error term###########################################
if(model==1)
{
eps       <-rnorm(N,0,10)#
}
if(model==2)
{
eps       <-rgamma(N,0.1,0.01)
eps<-scale(eps, scale = FALSE)
}
if(model==3)
{
eps       <-(X.2^2)*rnorm(N,0,10)#
}
############Different distribution####################################
if(model==4)
{
#Sample N random uniforms U
U =runif(N)
#Variable to store the samples from the mixture distribution
eps = rep(NA,N)
#Sampling from the mixture
for(i in 1:N){
if(U[i]<.3){
eps[i] = rnorm(1,-5.9,1)
}else if(U[i]<.8){
eps[i] = rnorm(1,4.1,1)
}else{
eps[i] = rnorm(1,-1.9,.1)
}
}
}
#Density plot of the random samples
plot(density(eps),main="Density Estimate of the Mixture Model")
####################################################################
if(model==5)
{
eps       <-  rt(N, df=1)#
}
plot(density(eps),main="Density Estimate of the t-distribution")
if(model==6)
{
#Sample N random uniforms U
#Variable to store the samples from the mixture distribution
eps =  runif(N,-1,1)
#Sampling from the mixture
}
###Heteroscedastic error term
beta.vec  <- c(1,1.5,-1.5,1.5)
y         <- X %*% beta.vec + eps
##Solving for beta hat###
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
x.grid<-sort(X.2) #seq(min(X.2),max(X.2),le=N)
true_model<-beta.vec[1]+beta.vec[2]*x.grid+beta.vec[3]*x.grid^2+beta.vec[4]*x.grid^3
estim_model<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
plot(estim_model,type="l")
lines(true_model,type="l",lty=1,col="red")
####Calculate the covariance matrix###
#####calculate the fitted values#####
y.hat<- X %*% beta.hat
eps.hat<-y-X %*% beta.hat
###calculate the covariance matrix#
se<-(t(eps.hat)%*%(eps.hat))/(N-5)
cov<-se[1]*solve(t(X) %*% X)
new_data<-X[order(X[,2]),]
#new_data<-x.grid
var<-c()
for(i in 1:N)
{
var[i]=t(new_data[i,])%*%cov%*%(new_data[i,])
}
d<-2*(sqrt(var))
estim<-beta.hat[1]+beta.hat[2]*new_data[,2]+beta.hat[3]*new_data[,2]^2+beta.hat[4]*new_data[,2]^3
conf_low<-estim-d
conf_high<-estim+d
plot(estim,type="l")
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
######Bootstrapped confidence intervals#####################
B=N
b_sample<-matrix(NA,N,B)
beta.hat<-matrix(NA,length(X),B)
data<-cbind(y,X)
function_B<-matrix(NA,length(x.grid),B)
for(i in 1:B)
{
XB<- data[sample(nrow(data),replace=T, N), ]
beta.hat<-solve(t(XB[,2:5]) %*% XB[,2:5]) %*% t(XB[,2:5]) %*%XB[,1]
function_B[,i]<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
}
plot(function_B[,1])
plot(function_B[,2])
sorted<-t(apply(function_B,1,sort))
####Calculate the empirical quantiles####################
conf_highB<-sorted[,B*0.95]
conf_lowB<-sorted[,B*0.05]
###########################################
plot(estim,type="l",lty=3)
lines(conf_highB)
lines(conf_lowB)
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
lines(true_model,col="green")
####Calculate the coverage probabilities#################################
cov_prob_B<-true_model>conf_lowB&true_model<conf_highB
ratio1_B= sum(cov_prob_B)/length(x.grid)
cov_prob_A<-true_model>conf_low&true_model<conf_high
ratio1=sum(cov_prob_A)/length(x.grid)
###Calculate interval length##############################################
int_lengthB<-conf_highB-conf_lowB
int_length<-conf_high-conf_low
ratio2=sum(int_lengthB/int_length>1)####counts the number of times the bootstrapped interval length is larger than the analytical one#########
ratio2_int=mean(int_length)
ratioB_int=mean(int_lengthB)
return(list(ratio2=ratio2,ratio1_B=ratio1_B,ratio1=ratio1,ratio2_int=ratio2_int,ratioB_int=ratioB_int))
}
##########
reps<-50
result_1<-replicate(reps,data.conf(model=1,N=200)$ratio1)
get_data <- function(N, p, mu, sigma, beta){
X <- rmvnorm(N, mean = mu, sigma = sigma)
epsilon <- rnorm(N, sd = 1)
Y <- X%*%beta + epsilon
return(list(X=X, Y=Y))
}
N. <- 1000
p. <- 200
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1, to = 2, length.out = p.))
beta. <- seq(0.1, 0.1, length.out = p.)
#beta. <- sample(c(0, seq(0.1, 0.5, le=1000)), prob = c(0.05, rep((1-0.05)/1000, 1000)), size = p., rep = TRUE)
# draw training sample
train_dat <- get_data(N., p., mu., sigma., beta.)
## a)
# grid for different lambdas
lambdas <- 10^seq(2, -2, by = -.1)
# ridge regression
# X matrix      # Y vector      # indicates ridge regression
ridge <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
summary(ridge)
# lasso
# X matrix      # Y vector      # indicates lasso regression
lasso <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
summary(lasso)
## b)
# generate test data set
test_dat <- get_data(N., p., mu., sigma., beta.)
# ridge:
# get Y predicted from ridge regression with the different lambdas
Y_predict_ridge <- predict(ridge, s = lambdas, newx = test_dat[[1]])
# prepare function for calculating the MSE
mse_func <- function(Y, Y_predict){
mean((Y - Y_predict)^2)
}
# prepare empty vector
mse_ridge <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_ridge[i] <- mse_func(test_dat[[2]], Y_predict_ridge[, i])
}
plot(lambdas, mse_ridge)
# lasso
# get Y predicted from ridge regression with the different lambdas
Y_predict_lasso <- predict(lasso, s = lambdas, newx = test_dat[[1]])
# prepare empty vector
mse_lasso <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_lasso[i] <- mse_func(test_dat[[2]], Y_predict_lasso[, i])
}
plot(lambdas, mse_lasso)
## c)
# find optimal lambda with cv.glmnet() wich uses k-fold cross validation to find best lambda / which
# minimizes MSE
cv_ridge <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
plot(cv_ridge)
# save optimal lambda
opt_lambda_ridge <- cv_ridge$lambda.min
cv_lasso <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
plot(cv_lasso)
# save optimal lambda
opt_lambda_lasso <- cv_lasso$lambda.min
# new test data set
test_dat2 <- get_data(N., p., mu., sigma., beta.)
# get predictions using optimal lambda
Y_predict_ridge2 <- predict(ridge, s = opt_lambda_ridge, newx = test_dat2[[1]])
Y_predict_lasso2 <- predict(lasso, s = opt_lambda_lasso, newx = test_dat2[[1]])
# OLS:
## fit the model
OLS <- lm(train_dat[[2]] ~ train_dat[[1]])
summary(OLS)
## get predictions
Y_predict_OLS <- predict(OLS, newx = test_dat2[[1]])
# MSE
mse_ridge2 <- mse_func(test_dat2[[2]], Y_predict_ridge2)
mse_lasso2 <- mse_func(test_dat2[[2]], Y_predict_lasso2)
mse_OLS <- mse_func(test_dat2[[2]], Y_predict_OLS)
cat(mse_ridge2, mse_lasso2, mse_OLS)
library(mvtnorm)
library(glmnet)
get_data <- function(N, p, mu, sigma, beta){
X <- rmvnorm(N, mean = mu, sigma = sigma)
epsilon <- rnorm(N, sd = 1)
Y <- X%*%beta + epsilon
return(list(X=X, Y=Y))
}
N. <- 1000
p. <- 200
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1, to = 2, length.out = p.))
beta. <- seq(0.1, 0.1, length.out = p.)
#beta. <- sample(c(0, seq(0.1, 0.5, le=1000)), prob = c(0.05, rep((1-0.05)/1000, 1000)), size = p., rep = TRUE)
# draw training sample
train_dat <- get_data(N., p., mu., sigma., beta.)
## a)
# grid for different lambdas
lambdas <- 10^seq(2, -2, by = -.1)
# ridge regression
# X matrix      # Y vector      # indicates ridge regression
ridge <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
summary(ridge)
# lasso
# X matrix      # Y vector      # indicates lasso regression
lasso <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
summary(lasso)
## b)
# generate test data set
test_dat <- get_data(N., p., mu., sigma., beta.)
# ridge:
# get Y predicted from ridge regression with the different lambdas
Y_predict_ridge <- predict(ridge, s = lambdas, newx = test_dat[[1]])
# prepare function for calculating the MSE
mse_func <- function(Y, Y_predict){
mean((Y - Y_predict)^2)
}
# prepare empty vector
mse_ridge <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_ridge[i] <- mse_func(test_dat[[2]], Y_predict_ridge[, i])
}
plot(lambdas, mse_ridge)
# lasso
# get Y predicted from ridge regression with the different lambdas
Y_predict_lasso <- predict(lasso, s = lambdas, newx = test_dat[[1]])
# prepare empty vector
mse_lasso <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_lasso[i] <- mse_func(test_dat[[2]], Y_predict_lasso[, i])
}
plot(lambdas, mse_lasso)
## c)
# find optimal lambda with cv.glmnet() wich uses k-fold cross validation to find best lambda / which
# minimizes MSE
cv_ridge <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
plot(cv_ridge)
# save optimal lambda
opt_lambda_ridge <- cv_ridge$lambda.min
cv_lasso <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
plot(cv_lasso)
# save optimal lambda
opt_lambda_lasso <- cv_lasso$lambda.min
# new test data set
test_dat2 <- get_data(N., p., mu., sigma., beta.)
# get predictions using optimal lambda
Y_predict_ridge2 <- predict(ridge, s = opt_lambda_ridge, newx = test_dat2[[1]])
Y_predict_lasso2 <- predict(lasso, s = opt_lambda_lasso, newx = test_dat2[[1]])
# OLS:
## fit the model
OLS <- lm(train_dat[[2]] ~ train_dat[[1]])
summary(OLS)
## get predictions
Y_predict_OLS <- predict(OLS, newx = test_dat2[[1]])
# MSE
mse_ridge2 <- mse_func(test_dat2[[2]], Y_predict_ridge2)
mse_lasso2 <- mse_func(test_dat2[[2]], Y_predict_lasso2)
mse_OLS <- mse_func(test_dat2[[2]], Y_predict_OLS)
cat(mse_ridge2, mse_lasso2, mse_OLS)
get_data <- function(N, p, mu, sigma, beta){
X <- rmvnorm(N, mean = mu, sigma = sigma)
epsilon <- rnorm(N, sd = 1)
Y <- X%*%beta + epsilon
return(list(X=X, Y=Y))
}
N. <- 1000
p. <- 200
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1, to = 2, length.out = p.))
beta. <- seq(3, 5, length.out = p.)
#beta. <- sample(c(0, seq(0.1, 0.5, le=1000)), prob = c(0.05, rep((1-0.05)/1000, 1000)), size = p., rep = TRUE)
# draw training sample
train_dat <- get_data(N., p., mu., sigma., beta.)
## a)
# grid for different lambdas
lambdas <- 10^seq(2, -2, by = -.1)
# ridge regression
# X matrix      # Y vector      # indicates ridge regression
ridge <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
summary(ridge)
# lasso
# X matrix      # Y vector      # indicates lasso regression
lasso <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
summary(lasso)
## b)
# generate test data set
test_dat <- get_data(N., p., mu., sigma., beta.)
# ridge:
# get Y predicted from ridge regression with the different lambdas
Y_predict_ridge <- predict(ridge, s = lambdas, newx = test_dat[[1]])
# prepare function for calculating the MSE
mse_func <- function(Y, Y_predict){
mean((Y - Y_predict)^2)
}
# prepare empty vector
mse_ridge <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_ridge[i] <- mse_func(test_dat[[2]], Y_predict_ridge[, i])
}
plot(lambdas, mse_ridge)
# lasso
# get Y predicted from ridge regression with the different lambdas
Y_predict_lasso <- predict(lasso, s = lambdas, newx = test_dat[[1]])
# prepare empty vector
mse_lasso <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_lasso[i] <- mse_func(test_dat[[2]], Y_predict_lasso[, i])
}
plot(lambdas, mse_lasso)
## c)
# find optimal lambda with cv.glmnet() wich uses k-fold cross validation to find best lambda / which
# minimizes MSE
cv_ridge <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
plot(cv_ridge)
# save optimal lambda
opt_lambda_ridge <- cv_ridge$lambda.min
cv_lasso <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
plot(cv_lasso)
# save optimal lambda
opt_lambda_lasso <- cv_lasso$lambda.min
# new test data set
test_dat2 <- get_data(N., p., mu., sigma., beta.)
# get predictions using optimal lambda
Y_predict_ridge2 <- predict(ridge, s = opt_lambda_ridge, newx = test_dat2[[1]])
Y_predict_lasso2 <- predict(lasso, s = opt_lambda_lasso, newx = test_dat2[[1]])
# OLS:
## fit the model
OLS <- lm(train_dat[[2]] ~ train_dat[[1]])
summary(OLS)
## get predictions
Y_predict_OLS <- predict(OLS, newx = test_dat2[[1]])
# MSE
mse_ridge2 <- mse_func(test_dat2[[2]], Y_predict_ridge2)
mse_lasso2 <- mse_func(test_dat2[[2]], Y_predict_lasso2)
mse_OLS <- mse_func(test_dat2[[2]], Y_predict_OLS)
cat(mse_ridge2, mse_lasso2, mse_OLS)
rm(list=ls())
setwd("C:\\Users\\Flori\\Documents\\Project_Microeconometrics\\student-project-timmens\\Simulation_Study\\helper_functions")
pack <- c("tidyverse", "plyr", "dplyr", "np", "FNN", "modelr", "rpart", "doParallel", "sandwich",
"grf", "Matching", "boot", "stargazer")
invisible(suppressMessages(suppressWarnings(lapply(pack, require, character.only = TRUE))))
?causal_forest
rm(list=ls())
#this should serve as a first step toward the simulation study.
library(tidyverse)
library(np)
library(dplyr)
library(plyr)
#library(glm)
library(modelr)
library(rpart)
library(FNN)
library(lattice)
setwd("C:\\Users\\Flori\\Documents\\Project_Microeconometrics\\student-project-timmens\\Simulation_Study\\helper_functions")
source("simAW.R")
tau <- function(x1,x2){
(1+(1+exp(-20*(x1-1/3)))^(-1))*(1+(1+exp(-20*(x2-1/3)))^(-1))
}
x1 <- seq(-5,5, length= 30)
x2 <- x1
z <- outer(x1, x2, tau)
?outer
z[is.na(z)] <- 1
#outer(c(1,2),c(3,4))
wireframe(z, drape=T, col.regions=rainbow(100),
xlab="x1", ylab="x2", zlab="tau")
n <- 2000
d <- 8
k <- 1
kC <- 50
dat <- simAW(n, kC, d)
summary(dat$CATE)
#not too bad.
summary(dat$tauhatknn)
summary(dat$Cate_ols)
#causal forest?
df <- dat
df_part <- modelr::resample_partition(df, c(train = 0.5, test = 0.5))
df_train <- as.data.frame(df_part$train)
df_test <- as.data.frame(df_part$test)
allX <- grep("^[X]", names(df), value=TRUE)
set.seed(1001)
cf <- grf::causal_forest(X = as.matrix(df_train[, allX]),
Y = as.matrix(df_train$Y_obs),
W = as.matrix(df_train$D),
num.trees = 500, # Make this larger for better acc.
num.threads = 1,
honesty = TRUE)
# Predict CATE and its std error for each individual on the dataset
cf_res <- predict(cf, df_test[, allX])
tauhatx_cf <- cf_res$predictions %>% as.numeric()
summary(tauhatx_cf)
plot(density(tauhatx_cf))
grf::plot.grf_tree(cf)
plot(cf)
MSE <- function(est, true){
y <- mean((est-true)^2)
}
mseknn <- MSE(df_test$tauhatknn, df_test$cate)
mseknn <- MSE(df_test$tauhatknn, df_test$CATE)
msecf <- MSE(tauhatx_cf, df_test$CATE)
mseols <- MSE(df_test$Cate_ols, df_test$CATE)
